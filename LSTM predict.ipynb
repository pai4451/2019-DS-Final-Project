{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 讀檔(已經預處理的pkl檔)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Read  data\n",
        "data = pd.read_pickle(\"./data/train_data.pkl\")\n",
        "data.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>air_store_id</th>\n",
              "      <th>2016-01-01</th>\n",
              "      <th>2016-01-02</th>\n",
              "      <th>2016-01-03</th>\n",
              "      <th>2016-01-04</th>\n",
              "      <th>2016-01-05</th>\n",
              "      <th>2016-01-06</th>\n",
              "      <th>2016-01-07</th>\n",
              "      <th>2016-01-08</th>\n",
              "      <th>2016-01-09</th>\n",
              "      <th>...</th>\n",
              "      <th>2017-04-17</th>\n",
              "      <th>2017-04-18</th>\n",
              "      <th>2017-04-19</th>\n",
              "      <th>2017-04-20</th>\n",
              "      <th>2017-04-21</th>\n",
              "      <th>2017-04-22</th>\n",
              "      <th>air_genre_name</th>\n",
              "      <th>air_area_name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>35</td>\n",
              "      <td>17</td>\n",
              "      <td>38</td>\n",
              "      <td>55</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>44</td>\n",
              "      <td>35.694003</td>\n",
              "      <td>139.753595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>air_0164b9927d20bcc3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>62</td>\n",
              "      <td>35.658068</td>\n",
              "      <td>139.751599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>air_0241aa3964b7f861</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>-1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>12</td>\n",
              "      <td>19</td>\n",
              "      <td>8</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>82</td>\n",
              "      <td>35.712607</td>\n",
              "      <td>139.779996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>air_0328696196e46f18</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>24</td>\n",
              "      <td>-1</td>\n",
              "      <td>19</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>98</td>\n",
              "      <td>34.701279</td>\n",
              "      <td>135.528090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>air_034a3d5b40d5b1b1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>25</td>\n",
              "      <td>20</td>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "      <td>37</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>102</td>\n",
              "      <td>34.692337</td>\n",
              "      <td>135.472229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 483 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           air_store_id  2016-01-01  2016-01-02  2016-01-03  2016-01-04  \\\n",
              "0  air_00a91d42b08b08d9          -1          -1          -1          -1   \n",
              "1  air_0164b9927d20bcc3          -1          -1          -1          -1   \n",
              "2  air_0241aa3964b7f861          -1          -1          10           9   \n",
              "3  air_0328696196e46f18          -1          -1          -1          -1   \n",
              "4  air_034a3d5b40d5b1b1          -1          -1          -1          -1   \n",
              "\n",
              "   2016-01-05  2016-01-06  2016-01-07  2016-01-08  2016-01-09  ...  \\\n",
              "0          -1          -1          -1          -1          -1  ...   \n",
              "1          -1          -1          -1          -1          -1  ...   \n",
              "2          17          10          -1           5           8  ...   \n",
              "3          -1          -1          -1          -1          -1  ...   \n",
              "4          -1          -1          -1          -1          -1  ...   \n",
              "\n",
              "   2017-04-17  2017-04-18  2017-04-19  2017-04-20  2017-04-21  2017-04-22  \\\n",
              "0          19          35          17          38          55          18   \n",
              "1           2           1           8           1          26           6   \n",
              "2          12          19           8          -1           3          13   \n",
              "3           3          -1          24          -1          19           8   \n",
              "4          25          20          31          12          37          35   \n",
              "\n",
              "   air_genre_name  air_area_name   latitude   longitude  \n",
              "0               6             44  35.694003  139.753595  \n",
              "1               6             62  35.658068  139.751599  \n",
              "2               7             82  35.712607  139.779996  \n",
              "3               4             98  34.701279  135.528090  \n",
              "4               2            102  34.692337  135.472229  \n",
              "\n",
              "[5 rows x 483 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nomalization\n",
        "\n",
        "把原本data = -1的用0替代，其餘非0的數值利用\n",
        "$x' = \\ln (1+x)$轉換"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def log_transform(data):\n",
        "    data = data.applymap(lambda x: 0 if x==-1 else x)\n",
        "    data.iloc[:, 1:479] = np.log1p(data.iloc[:, 1:479])\n",
        "    return data\n",
        "\n",
        "data = log_transform(data)\n",
        "data.head(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>air_store_id</th>\n",
              "      <th>2016-01-01</th>\n",
              "      <th>2016-01-02</th>\n",
              "      <th>2016-01-03</th>\n",
              "      <th>2016-01-04</th>\n",
              "      <th>2016-01-05</th>\n",
              "      <th>2016-01-06</th>\n",
              "      <th>2016-01-07</th>\n",
              "      <th>2016-01-08</th>\n",
              "      <th>2016-01-09</th>\n",
              "      <th>...</th>\n",
              "      <th>2017-04-17</th>\n",
              "      <th>2017-04-18</th>\n",
              "      <th>2017-04-19</th>\n",
              "      <th>2017-04-20</th>\n",
              "      <th>2017-04-21</th>\n",
              "      <th>2017-04-22</th>\n",
              "      <th>air_genre_name</th>\n",
              "      <th>air_area_name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>3.583519</td>\n",
              "      <td>2.890372</td>\n",
              "      <td>3.663562</td>\n",
              "      <td>4.025352</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>6</td>\n",
              "      <td>44</td>\n",
              "      <td>35.694003</td>\n",
              "      <td>139.753595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>air_0164b9927d20bcc3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>3.295837</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>6</td>\n",
              "      <td>62</td>\n",
              "      <td>35.658068</td>\n",
              "      <td>139.751599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>air_0241aa3964b7f861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.397895</td>\n",
              "      <td>2.302585</td>\n",
              "      <td>2.890372</td>\n",
              "      <td>2.397895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>...</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>7</td>\n",
              "      <td>82</td>\n",
              "      <td>35.712607</td>\n",
              "      <td>139.779996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>air_0328696196e46f18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.218876</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>4</td>\n",
              "      <td>98</td>\n",
              "      <td>34.701279</td>\n",
              "      <td>135.528090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>air_034a3d5b40d5b1b1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.258097</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>3.465736</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>3.583519</td>\n",
              "      <td>2</td>\n",
              "      <td>102</td>\n",
              "      <td>34.692337</td>\n",
              "      <td>135.472229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>air_036d4f1ee7285390</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>3.496508</td>\n",
              "      <td>3.970292</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>3.401197</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>34.799767</td>\n",
              "      <td>135.360073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>air_0382c794b73b51ad</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>...</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>3.295837</td>\n",
              "      <td>3.135494</td>\n",
              "      <td>3.218876</td>\n",
              "      <td>3.218876</td>\n",
              "      <td>3.713572</td>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>35.602125</td>\n",
              "      <td>139.671958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>air_03963426c9312048</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.143135</td>\n",
              "      <td>3.433987</td>\n",
              "      <td>3.970292</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>4.499810</td>\n",
              "      <td>4.110874</td>\n",
              "      <td>...</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>3.295837</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>3.258097</td>\n",
              "      <td>4.330733</td>\n",
              "      <td>4.060443</td>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>34.386245</td>\n",
              "      <td>132.455018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>air_04341b588bde96cd</td>\n",
              "      <td>2.397895</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>3.583519</td>\n",
              "      <td>3.526361</td>\n",
              "      <td>3.970292</td>\n",
              "      <td>3.871201</td>\n",
              "      <td>4.276666</td>\n",
              "      <td>...</td>\n",
              "      <td>3.465736</td>\n",
              "      <td>3.555348</td>\n",
              "      <td>3.583519</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>3.871201</td>\n",
              "      <td>3.828641</td>\n",
              "      <td>7</td>\n",
              "      <td>66</td>\n",
              "      <td>35.735623</td>\n",
              "      <td>139.651658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>air_049f6d5b402a31b2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>2.708050</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>2.833213</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>3.401197</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>33.589216</td>\n",
              "      <td>130.392813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 483 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           air_store_id  2016-01-01  2016-01-02  2016-01-03  2016-01-04  \\\n",
              "0  air_00a91d42b08b08d9    0.000000         0.0    0.000000    0.000000   \n",
              "1  air_0164b9927d20bcc3    0.000000         0.0    0.000000    0.000000   \n",
              "2  air_0241aa3964b7f861    0.000000         0.0    2.397895    2.302585   \n",
              "3  air_0328696196e46f18    0.000000         0.0    0.000000    0.000000   \n",
              "4  air_034a3d5b40d5b1b1    0.000000         0.0    0.000000    0.000000   \n",
              "5  air_036d4f1ee7285390    0.000000         0.0    0.000000    0.000000   \n",
              "6  air_0382c794b73b51ad    0.000000         0.0    0.000000    0.000000   \n",
              "7  air_03963426c9312048    0.000000         0.0    0.000000    4.143135   \n",
              "8  air_04341b588bde96cd    2.397895         0.0    0.000000    3.178054   \n",
              "9  air_049f6d5b402a31b2    0.000000         0.0    0.000000    0.000000   \n",
              "\n",
              "   2016-01-05  2016-01-06  2016-01-07  2016-01-08  2016-01-09  ...  \\\n",
              "0    0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
              "1    0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
              "2    2.890372    2.397895    0.000000    1.791759    2.197225  ...   \n",
              "3    0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
              "4    0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
              "5    0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
              "6    0.000000    0.000000    0.000000    0.000000    1.386294  ...   \n",
              "7    3.433987    3.970292    3.637586    4.499810    4.110874  ...   \n",
              "8    3.583519    3.526361    3.970292    3.871201    4.276666  ...   \n",
              "9    0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
              "\n",
              "   2017-04-17  2017-04-18  2017-04-19  2017-04-20  2017-04-21  2017-04-22  \\\n",
              "0    2.995732    3.583519    2.890372    3.663562    4.025352    2.944439   \n",
              "1    1.098612    0.693147    2.197225    0.693147    3.295837    1.945910   \n",
              "2    2.564949    2.995732    2.197225    0.000000    1.386294    2.639057   \n",
              "3    1.386294    0.000000    3.218876    0.000000    2.995732    2.197225   \n",
              "4    3.258097    3.044522    3.465736    2.564949    3.637586    3.583519   \n",
              "5    1.945910    3.496508    3.970292    2.944439    3.401197    3.178054   \n",
              "6    3.367296    3.295837    3.135494    3.218876    3.218876    3.713572   \n",
              "7    3.637586    3.295837    3.178054    3.258097    4.330733    4.060443   \n",
              "8    3.465736    3.555348    3.583519    2.197225    3.871201    3.828641   \n",
              "9    2.197225    2.708050    2.564949    2.833213    2.639057    3.401197   \n",
              "\n",
              "   air_genre_name  air_area_name   latitude   longitude  \n",
              "0               6             44  35.694003  139.753595  \n",
              "1               6             62  35.658068  139.751599  \n",
              "2               7             82  35.712607  139.779996  \n",
              "3               4             98  34.701279  135.528090  \n",
              "4               2            102  34.692337  135.472229  \n",
              "5               2             31  34.799767  135.360073  \n",
              "6               2             68  35.602125  139.671958  \n",
              "7               7             15  34.386245  132.455018  \n",
              "8               7             66  35.735623  139.651658  \n",
              "9               8              0  33.589216  130.392813  \n",
              "\n",
              "[10 rows x 483 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 產生lstm input (shape = # , 39, 821)\n",
        "因為`sample_submission`檔案裡面只有821個不同的air_data\n",
        "產生連續39天的資料，前39天當$\\mathbf{x}$，後39天當$y$<br>\n",
        "例如用2016-1-1~2016-2-8作為$\\mathbf{x}$ 2016-2-9~2016-3-18作為$y$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_walkforward_data(data, input_seq_len, output_seq_len, date_start_col, date_end_col, val_data=0.05):\n",
        "    date_cols = np.r_[date_start_col: date_end_col]\n",
        "    train_points = len(date_cols) - input_seq_len - output_seq_len\n",
        "    input_seqs = []\n",
        "    output_seqs = []\n",
        "    for i in range(train_points+1):\n",
        "        inp_start = i + date_start_col\n",
        "        inp_end = inp_start + input_seq_len\n",
        "        out_end = inp_end + output_seq_len \n",
        "        input_seqs.append( data.iloc[: , inp_start:inp_end].values.reshape(1, -1, input_seq_len).transpose(0, 2, 1) )\n",
        "        output_seqs.append( data.iloc[: , inp_end:out_end].values.reshape(1, -1, output_seq_len).transpose(0, 2, 1) )\n",
        "        \n",
        "    input_seqs = np.concatenate(input_seqs)\n",
        "    output_seqs = np.concatenate(output_seqs)\n",
        "    \n",
        "    train_x, val_x, train_y, val_y = train_test_split(input_seqs, output_seqs, test_size=val_data, random_state=1126)\n",
        "    \n",
        "    return train_x, val_x, train_y, val_y"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq_len = 39\n",
        "output_seq_len = 39\n",
        "train_x, val_x, train_y, val_y = generate_walkforward_data(data, input_seq_len, output_seq_len, 1, 479)\n",
        "print(train_x.shape, train_y.shape, val_x.shape, val_y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(380, 39, 821) (380, 39, 821) (21, 39, 821) (21, 39, 821)\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n",
            "C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def buildManyToManyModel(train_data):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(10, input_shape = (train_data.shape[1], train_data.shape[2]),return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(821)))\n",
        "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = buildManyToManyModel(train_x)\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 39, 10)            33280     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 39, 821)           9031      \n",
            "=================================================================\n",
            "Total params: 42,311\n",
            "Trainable params: 42,311\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "callback = EarlyStopping(monitor=\"loss\", patience=3, verbose=1, mode=\"auto\")\n",
        "history = model.fit(train_x, train_y, epochs=1000, batch_size=50, validation_data=(val_x, val_y), callbacks=[callback])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\yaoweipai\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 380 samples, validate on 21 samples\n",
            "Epoch 1/1000\n",
            "380/380 [==============================] - 2s 6ms/step - loss: 5.6714 - val_loss: 5.0616\n",
            "Epoch 2/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 5.3948 - val_loss: 4.7974\n",
            "Epoch 3/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 5.1057 - val_loss: 4.5506\n",
            "Epoch 4/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 4.8369 - val_loss: 4.3170\n",
            "Epoch 5/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 4.5846 - val_loss: 4.0990\n",
            "Epoch 6/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 4.3476 - val_loss: 3.8980\n",
            "Epoch 7/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 4.1289 - val_loss: 3.7129\n",
            "Epoch 8/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 3.9264 - val_loss: 3.5425\n",
            "Epoch 9/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 3.7380 - val_loss: 3.3854\n",
            "Epoch 10/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 3.5654 - val_loss: 3.2406\n",
            "Epoch 11/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 3.4073 - val_loss: 3.1067\n",
            "Epoch 12/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 3.2579 - val_loss: 2.9847\n",
            "Epoch 13/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 3.1215 - val_loss: 2.8727\n",
            "Epoch 14/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.9964 - val_loss: 2.7696\n",
            "Epoch 15/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.8797 - val_loss: 2.6752\n",
            "Epoch 16/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.7742 - val_loss: 2.5880\n",
            "Epoch 17/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.6749 - val_loss: 2.5084\n",
            "Epoch 18/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.5838 - val_loss: 2.4359\n",
            "Epoch 19/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.5022 - val_loss: 2.3691\n",
            "Epoch 20/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.4246 - val_loss: 2.3086\n",
            "Epoch 21/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.3548 - val_loss: 2.2531\n",
            "Epoch 22/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.2908 - val_loss: 2.2023\n",
            "Epoch 23/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.2315 - val_loss: 2.1558\n",
            "Epoch 24/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.1768 - val_loss: 2.1134\n",
            "Epoch 25/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.1276 - val_loss: 2.0746\n",
            "Epoch 26/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.0818 - val_loss: 2.0393\n",
            "Epoch 27/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.0397 - val_loss: 2.0069\n",
            "Epoch 28/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 2.0020 - val_loss: 1.9774\n",
            "Epoch 29/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.9664 - val_loss: 1.9504\n",
            "Epoch 30/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.9342 - val_loss: 1.9256\n",
            "Epoch 31/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.9049 - val_loss: 1.9033\n",
            "Epoch 32/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.8774 - val_loss: 1.8828\n",
            "Epoch 33/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.8526 - val_loss: 1.8638\n",
            "Epoch 34/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.8299 - val_loss: 1.8465\n",
            "Epoch 35/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.8068 - val_loss: 1.8191\n",
            "Epoch 36/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.7854 - val_loss: 1.8040\n",
            "Epoch 37/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.7666 - val_loss: 1.7899\n",
            "Epoch 38/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.7495 - val_loss: 1.7771\n",
            "Epoch 39/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.7318 - val_loss: 1.7204\n",
            "Epoch 40/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.6991 - val_loss: 1.6896\n",
            "Epoch 41/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.6666 - val_loss: 1.6718\n",
            "Epoch 42/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.6409 - val_loss: 1.6437\n",
            "Epoch 43/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.6179 - val_loss: 1.6238\n",
            "Epoch 44/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.5974 - val_loss: 1.6119\n",
            "Epoch 45/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.5773 - val_loss: 1.5910\n",
            "Epoch 46/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.5587 - val_loss: 1.5694\n",
            "Epoch 47/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.5414 - val_loss: 1.5550\n",
            "Epoch 48/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.5247 - val_loss: 1.5382\n",
            "Epoch 49/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.5094 - val_loss: 1.5230\n",
            "Epoch 50/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4949 - val_loss: 1.5085\n",
            "Epoch 51/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4814 - val_loss: 1.4945\n",
            "Epoch 52/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4687 - val_loss: 1.4820\n",
            "Epoch 53/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4563 - val_loss: 1.4694\n",
            "Epoch 54/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4451 - val_loss: 1.4566\n",
            "Epoch 55/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4340 - val_loss: 1.4450\n",
            "Epoch 56/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4236 - val_loss: 1.4337\n",
            "Epoch 57/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4132 - val_loss: 1.4234\n",
            "Epoch 58/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.4037 - val_loss: 1.4127\n",
            "Epoch 59/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3947 - val_loss: 1.4022\n",
            "Epoch 60/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3858 - val_loss: 1.3926\n",
            "Epoch 61/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3772 - val_loss: 1.3823\n",
            "Epoch 62/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3689 - val_loss: 1.3723\n",
            "Epoch 63/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3581 - val_loss: 1.3555\n",
            "Epoch 64/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3492 - val_loss: 1.3473\n",
            "Epoch 65/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3404 - val_loss: 1.3374\n",
            "Epoch 66/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3332 - val_loss: 1.3289\n",
            "Epoch 67/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3229 - val_loss: 1.3060\n",
            "Epoch 68/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3077 - val_loss: 1.3004\n",
            "Epoch 69/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.3002 - val_loss: 1.2877\n",
            "Epoch 70/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2913 - val_loss: 1.2725\n",
            "Epoch 71/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2824 - val_loss: 1.2648\n",
            "Epoch 72/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2757 - val_loss: 1.2571\n",
            "Epoch 73/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2678 - val_loss: 1.2487\n",
            "Epoch 74/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2612 - val_loss: 1.2408\n",
            "Epoch 75/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2546 - val_loss: 1.2331\n",
            "Epoch 76/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2477 - val_loss: 1.2253\n",
            "Epoch 77/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2418 - val_loss: 1.2181\n",
            "Epoch 78/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2358 - val_loss: 1.2109\n",
            "Epoch 79/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2302 - val_loss: 1.2041\n",
            "Epoch 80/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2245 - val_loss: 1.1972\n",
            "Epoch 81/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2192 - val_loss: 1.1902\n",
            "Epoch 82/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2139 - val_loss: 1.1834\n",
            "Epoch 83/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2086 - val_loss: 1.1768\n",
            "Epoch 84/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.2036 - val_loss: 1.1705\n",
            "Epoch 85/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1987 - val_loss: 1.1641\n",
            "Epoch 86/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1939 - val_loss: 1.1580\n",
            "Epoch 87/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1892 - val_loss: 1.1521\n",
            "Epoch 88/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1845 - val_loss: 1.1466\n",
            "Epoch 89/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1801 - val_loss: 1.1409\n",
            "Epoch 90/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1756 - val_loss: 1.1351\n",
            "Epoch 91/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1713 - val_loss: 1.1294\n",
            "Epoch 92/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1670 - val_loss: 1.1239\n",
            "Epoch 93/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1628 - val_loss: 1.1185\n",
            "Epoch 94/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1587 - val_loss: 1.1133\n",
            "Epoch 95/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1547 - val_loss: 1.1081\n",
            "Epoch 96/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1507 - val_loss: 1.1030\n",
            "Epoch 97/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1465 - val_loss: 1.0981\n",
            "Epoch 98/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1421 - val_loss: 1.0930\n",
            "Epoch 99/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1383 - val_loss: 1.0883\n",
            "Epoch 100/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1347 - val_loss: 1.0836\n",
            "Epoch 101/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1312 - val_loss: 1.0789\n",
            "Epoch 102/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1277 - val_loss: 1.0744\n",
            "Epoch 103/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1242 - val_loss: 1.0702\n",
            "Epoch 104/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1210 - val_loss: 1.0658\n",
            "Epoch 105/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1178 - val_loss: 1.0613\n",
            "Epoch 106/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1145 - val_loss: 1.0572\n",
            "Epoch 107/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1114 - val_loss: 1.0533\n",
            "Epoch 108/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1083 - val_loss: 1.0495\n",
            "Epoch 109/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1054 - val_loss: 1.0457\n",
            "Epoch 110/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.1025 - val_loss: 1.0419\n",
            "Epoch 111/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0996 - val_loss: 1.0380\n",
            "Epoch 112/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0967 - val_loss: 1.0344\n",
            "Epoch 113/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0940 - val_loss: 1.0306\n",
            "Epoch 114/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0912 - val_loss: 1.0271\n",
            "Epoch 115/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0885 - val_loss: 1.0237\n",
            "Epoch 116/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0859 - val_loss: 1.0203\n",
            "Epoch 117/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0834 - val_loss: 1.0168\n",
            "Epoch 118/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0808 - val_loss: 1.0136\n",
            "Epoch 119/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0783 - val_loss: 1.0105\n",
            "Epoch 120/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0760 - val_loss: 1.0074\n",
            "Epoch 121/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0736 - val_loss: 1.0042\n",
            "Epoch 122/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0713 - val_loss: 1.0012\n",
            "Epoch 123/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0690 - val_loss: 0.9983\n",
            "Epoch 124/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0668 - val_loss: 0.9955\n",
            "Epoch 125/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0646 - val_loss: 0.9927\n",
            "Epoch 126/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0624 - val_loss: 0.9899\n",
            "Epoch 127/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0604 - val_loss: 0.9871\n",
            "Epoch 128/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0584 - val_loss: 0.9844\n",
            "Epoch 129/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0563 - val_loss: 0.9819\n",
            "Epoch 130/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0544 - val_loss: 0.9795\n",
            "Epoch 131/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0526 - val_loss: 0.9769\n",
            "Epoch 132/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0506 - val_loss: 0.9744\n",
            "Epoch 133/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0487 - val_loss: 0.9720\n",
            "Epoch 134/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0469 - val_loss: 0.9697\n",
            "Epoch 135/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0451 - val_loss: 0.9674\n",
            "Epoch 136/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0433 - val_loss: 0.9650\n",
            "Epoch 137/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0416 - val_loss: 0.9628\n",
            "Epoch 138/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0399 - val_loss: 0.9607\n",
            "Epoch 139/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0383 - val_loss: 0.9585\n",
            "Epoch 140/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0367 - val_loss: 0.9564\n",
            "Epoch 141/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0352 - val_loss: 0.9542\n",
            "Epoch 142/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0336 - val_loss: 0.9524\n",
            "Epoch 143/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0321 - val_loss: 0.9504\n",
            "Epoch 144/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0306 - val_loss: 0.9486\n",
            "Epoch 145/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0292 - val_loss: 0.9466\n",
            "Epoch 146/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0278 - val_loss: 0.9449\n",
            "Epoch 147/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0264 - val_loss: 0.9432\n",
            "Epoch 148/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0251 - val_loss: 0.9414\n",
            "Epoch 149/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0237 - val_loss: 0.9397\n",
            "Epoch 150/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0224 - val_loss: 0.9379\n",
            "Epoch 151/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0212 - val_loss: 0.9365\n",
            "Epoch 152/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0200 - val_loss: 0.9347\n",
            "Epoch 153/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0187 - val_loss: 0.9332\n",
            "Epoch 154/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0175 - val_loss: 0.9316\n",
            "Epoch 155/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0163 - val_loss: 0.9302\n",
            "Epoch 156/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0152 - val_loss: 0.9287\n",
            "Epoch 157/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0141 - val_loss: 0.9272\n",
            "Epoch 158/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0130 - val_loss: 0.9260\n",
            "Epoch 159/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0120 - val_loss: 0.9246\n",
            "Epoch 160/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0110 - val_loss: 0.9233\n",
            "Epoch 161/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0099 - val_loss: 0.9220\n",
            "Epoch 162/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0089 - val_loss: 0.9207\n",
            "Epoch 163/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0080 - val_loss: 0.9193\n",
            "Epoch 164/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0070 - val_loss: 0.9183\n",
            "Epoch 165/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0061 - val_loss: 0.9170\n",
            "Epoch 166/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0052 - val_loss: 0.9159\n",
            "Epoch 167/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0043 - val_loss: 0.9148\n",
            "Epoch 168/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0034 - val_loss: 0.9135\n",
            "Epoch 169/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0026 - val_loss: 0.9124\n",
            "Epoch 170/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0017 - val_loss: 0.9114\n",
            "Epoch 171/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0009 - val_loss: 0.9103\n",
            "Epoch 172/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 1.0001 - val_loss: 0.9095\n",
            "Epoch 173/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9994 - val_loss: 0.9085\n",
            "Epoch 174/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9986 - val_loss: 0.9076\n",
            "Epoch 175/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9979 - val_loss: 0.9065\n",
            "Epoch 176/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9971 - val_loss: 0.9056\n",
            "Epoch 177/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9964 - val_loss: 0.9047\n",
            "Epoch 178/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9957 - val_loss: 0.9039\n",
            "Epoch 179/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9950 - val_loss: 0.9030\n",
            "Epoch 180/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9944 - val_loss: 0.9022\n",
            "Epoch 181/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9937 - val_loss: 0.9014\n",
            "Epoch 182/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9931 - val_loss: 0.9006\n",
            "Epoch 183/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9925 - val_loss: 0.8998\n",
            "Epoch 184/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9919 - val_loss: 0.8990\n",
            "Epoch 185/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9913 - val_loss: 0.8982\n",
            "Epoch 186/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9907 - val_loss: 0.8975\n",
            "Epoch 187/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9901 - val_loss: 0.8968\n",
            "Epoch 188/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9896 - val_loss: 0.8960\n",
            "Epoch 189/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9890 - val_loss: 0.8954\n",
            "Epoch 190/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9885 - val_loss: 0.8946\n",
            "Epoch 191/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9880 - val_loss: 0.8940\n",
            "Epoch 192/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9874 - val_loss: 0.8934\n",
            "Epoch 193/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9869 - val_loss: 0.8928\n",
            "Epoch 194/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9864 - val_loss: 0.8921\n",
            "Epoch 195/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9860 - val_loss: 0.8916\n",
            "Epoch 196/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9855 - val_loss: 0.8911\n",
            "Epoch 197/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9850 - val_loss: 0.8905\n",
            "Epoch 198/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9846 - val_loss: 0.8899\n",
            "Epoch 199/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9841 - val_loss: 0.8893\n",
            "Epoch 200/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9837 - val_loss: 0.8889\n",
            "Epoch 201/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9832 - val_loss: 0.8884\n",
            "Epoch 202/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9828 - val_loss: 0.8879\n",
            "Epoch 203/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9824 - val_loss: 0.8874\n",
            "Epoch 204/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9820 - val_loss: 0.8869\n",
            "Epoch 205/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9816 - val_loss: 0.8864\n",
            "Epoch 206/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9813 - val_loss: 0.8859\n",
            "Epoch 207/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9809 - val_loss: 0.8855\n",
            "Epoch 208/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9805 - val_loss: 0.8851\n",
            "Epoch 209/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9801 - val_loss: 0.8846\n",
            "Epoch 210/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9798 - val_loss: 0.8842\n",
            "Epoch 211/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9794 - val_loss: 0.8838\n",
            "Epoch 212/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9791 - val_loss: 0.8834\n",
            "Epoch 213/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9788 - val_loss: 0.8830\n",
            "Epoch 214/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9784 - val_loss: 0.8826\n",
            "Epoch 215/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9781 - val_loss: 0.8822\n",
            "Epoch 216/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9778 - val_loss: 0.8818\n",
            "Epoch 217/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9775 - val_loss: 0.8815\n",
            "Epoch 218/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9772 - val_loss: 0.8811\n",
            "Epoch 219/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9769 - val_loss: 0.8809\n",
            "Epoch 220/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9766 - val_loss: 0.8805\n",
            "Epoch 221/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9763 - val_loss: 0.8802\n",
            "Epoch 222/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9760 - val_loss: 0.8798\n",
            "Epoch 223/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9757 - val_loss: 0.8794\n",
            "Epoch 224/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9754 - val_loss: 0.8792\n",
            "Epoch 225/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9752 - val_loss: 0.8788\n",
            "Epoch 226/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9749 - val_loss: 0.8785\n",
            "Epoch 227/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9746 - val_loss: 0.8781\n",
            "Epoch 228/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9744 - val_loss: 0.8778\n",
            "Epoch 229/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9741 - val_loss: 0.8776\n",
            "Epoch 230/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9739 - val_loss: 0.8774\n",
            "Epoch 231/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9736 - val_loss: 0.8772\n",
            "Epoch 232/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9734 - val_loss: 0.8770\n",
            "Epoch 233/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9731 - val_loss: 0.8766\n",
            "Epoch 234/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9729 - val_loss: 0.8763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 235/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9727 - val_loss: 0.8761\n",
            "Epoch 236/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9724 - val_loss: 0.8758\n",
            "Epoch 237/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9722 - val_loss: 0.8755\n",
            "Epoch 238/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9720 - val_loss: 0.8753\n",
            "Epoch 239/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9718 - val_loss: 0.8751\n",
            "Epoch 240/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9716 - val_loss: 0.8748\n",
            "Epoch 241/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9714 - val_loss: 0.8746\n",
            "Epoch 242/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9712 - val_loss: 0.8745\n",
            "Epoch 243/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9709 - val_loss: 0.8743\n",
            "Epoch 244/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9708 - val_loss: 0.8741\n",
            "Epoch 245/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9706 - val_loss: 0.8737\n",
            "Epoch 246/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9703 - val_loss: 0.8735\n",
            "Epoch 247/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9701 - val_loss: 0.8732\n",
            "Epoch 248/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9700 - val_loss: 0.8732\n",
            "Epoch 249/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9698 - val_loss: 0.8729\n",
            "Epoch 250/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9696 - val_loss: 0.8727\n",
            "Epoch 251/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9694 - val_loss: 0.8724\n",
            "Epoch 252/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9692 - val_loss: 0.8723\n",
            "Epoch 253/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9691 - val_loss: 0.8722\n",
            "Epoch 254/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9689 - val_loss: 0.8719\n",
            "Epoch 255/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9687 - val_loss: 0.8717\n",
            "Epoch 256/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9685 - val_loss: 0.8715\n",
            "Epoch 257/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9683 - val_loss: 0.8714\n",
            "Epoch 258/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9682 - val_loss: 0.8711\n",
            "Epoch 259/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9680 - val_loss: 0.8710\n",
            "Epoch 260/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9678 - val_loss: 0.8708\n",
            "Epoch 261/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9677 - val_loss: 0.8708\n",
            "Epoch 262/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9675 - val_loss: 0.8706\n",
            "Epoch 263/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9673 - val_loss: 0.8705\n",
            "Epoch 264/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9672 - val_loss: 0.8703\n",
            "Epoch 265/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9670 - val_loss: 0.8701\n",
            "Epoch 266/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9669 - val_loss: 0.8699\n",
            "Epoch 267/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9667 - val_loss: 0.8697\n",
            "Epoch 268/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9666 - val_loss: 0.8697\n",
            "Epoch 269/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9664 - val_loss: 0.8695\n",
            "Epoch 270/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9662 - val_loss: 0.8694\n",
            "Epoch 271/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9661 - val_loss: 0.8692\n",
            "Epoch 272/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9660 - val_loss: 0.8689\n",
            "Epoch 273/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9658 - val_loss: 0.8689\n",
            "Epoch 274/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9657 - val_loss: 0.8689\n",
            "Epoch 275/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9655 - val_loss: 0.8687\n",
            "Epoch 276/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9654 - val_loss: 0.8686\n",
            "Epoch 277/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9652 - val_loss: 0.8684\n",
            "Epoch 278/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9651 - val_loss: 0.8683\n",
            "Epoch 279/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9649 - val_loss: 0.8681\n",
            "Epoch 280/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9648 - val_loss: 0.8680\n",
            "Epoch 281/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9647 - val_loss: 0.8678\n",
            "Epoch 282/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9645 - val_loss: 0.8678\n",
            "Epoch 283/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9644 - val_loss: 0.8677\n",
            "Epoch 284/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9642 - val_loss: 0.8676\n",
            "Epoch 285/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9641 - val_loss: 0.8674\n",
            "Epoch 286/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9640 - val_loss: 0.8672\n",
            "Epoch 287/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9638 - val_loss: 0.8672\n",
            "Epoch 288/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9637 - val_loss: 0.8670\n",
            "Epoch 289/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9636 - val_loss: 0.8669\n",
            "Epoch 290/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9634 - val_loss: 0.8668\n",
            "Epoch 291/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9633 - val_loss: 0.8667\n",
            "Epoch 292/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9632 - val_loss: 0.8665\n",
            "Epoch 293/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9630 - val_loss: 0.8664\n",
            "Epoch 294/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9629 - val_loss: 0.8663\n",
            "Epoch 295/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9628 - val_loss: 0.8662\n",
            "Epoch 296/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9626 - val_loss: 0.8661\n",
            "Epoch 297/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9625 - val_loss: 0.8660\n",
            "Epoch 298/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9624 - val_loss: 0.8659\n",
            "Epoch 299/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9623 - val_loss: 0.8657\n",
            "Epoch 300/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9622 - val_loss: 0.8658\n",
            "Epoch 301/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9620 - val_loss: 0.8657\n",
            "Epoch 302/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9619 - val_loss: 0.8658\n",
            "Epoch 303/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9618 - val_loss: 0.8655\n",
            "Epoch 304/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9616 - val_loss: 0.8654\n",
            "Epoch 305/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9615 - val_loss: 0.8651\n",
            "Epoch 306/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9614 - val_loss: 0.8650\n",
            "Epoch 307/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9613 - val_loss: 0.8649\n",
            "Epoch 308/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9611 - val_loss: 0.8649\n",
            "Epoch 309/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9610 - val_loss: 0.8648\n",
            "Epoch 310/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9609 - val_loss: 0.8648\n",
            "Epoch 311/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9608 - val_loss: 0.8647\n",
            "Epoch 312/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9607 - val_loss: 0.8644\n",
            "Epoch 313/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9606 - val_loss: 0.8643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 314/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9605 - val_loss: 0.8641\n",
            "Epoch 315/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9603 - val_loss: 0.8640\n",
            "Epoch 316/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9602 - val_loss: 0.8640\n",
            "Epoch 317/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9601 - val_loss: 0.8640\n",
            "Epoch 318/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9600 - val_loss: 0.8639\n",
            "Epoch 319/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9599 - val_loss: 0.8638\n",
            "Epoch 320/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9597 - val_loss: 0.8635\n",
            "Epoch 321/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9596 - val_loss: 0.8635\n",
            "Epoch 322/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9595 - val_loss: 0.8633\n",
            "Epoch 323/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9594 - val_loss: 0.8633\n",
            "Epoch 324/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9593 - val_loss: 0.8633\n",
            "Epoch 325/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9592 - val_loss: 0.8632\n",
            "Epoch 326/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9591 - val_loss: 0.8630\n",
            "Epoch 327/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9589 - val_loss: 0.8629\n",
            "Epoch 328/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9588 - val_loss: 0.8629\n",
            "Epoch 329/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9587 - val_loss: 0.8628\n",
            "Epoch 330/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9586 - val_loss: 0.8627\n",
            "Epoch 331/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9585 - val_loss: 0.8627\n",
            "Epoch 332/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9584 - val_loss: 0.8625\n",
            "Epoch 333/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9583 - val_loss: 0.8626\n",
            "Epoch 334/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9582 - val_loss: 0.8624\n",
            "Epoch 335/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9581 - val_loss: 0.8623\n",
            "Epoch 336/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9580 - val_loss: 0.8622\n",
            "Epoch 337/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9578 - val_loss: 0.8621\n",
            "Epoch 338/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9578 - val_loss: 0.8621\n",
            "Epoch 339/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9577 - val_loss: 0.8619\n",
            "Epoch 340/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9575 - val_loss: 0.8618\n",
            "Epoch 341/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9574 - val_loss: 0.8618\n",
            "Epoch 342/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9573 - val_loss: 0.8617\n",
            "Epoch 343/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9572 - val_loss: 0.8616\n",
            "Epoch 344/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9571 - val_loss: 0.8616\n",
            "Epoch 345/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9570 - val_loss: 0.8616\n",
            "Epoch 346/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9569 - val_loss: 0.8615\n",
            "Epoch 347/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9568 - val_loss: 0.8613\n",
            "Epoch 348/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9567 - val_loss: 0.8611\n",
            "Epoch 349/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9566 - val_loss: 0.8609\n",
            "Epoch 350/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9565 - val_loss: 0.8611\n",
            "Epoch 351/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9564 - val_loss: 0.8609\n",
            "Epoch 352/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9563 - val_loss: 0.8609\n",
            "Epoch 353/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9562 - val_loss: 0.8609\n",
            "Epoch 354/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9561 - val_loss: 0.8607\n",
            "Epoch 355/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9560 - val_loss: 0.8606\n",
            "Epoch 356/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9559 - val_loss: 0.8604\n",
            "Epoch 357/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9557 - val_loss: 0.8603\n",
            "Epoch 358/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9557 - val_loss: 0.8603\n",
            "Epoch 359/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9556 - val_loss: 0.8603\n",
            "Epoch 360/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9555 - val_loss: 0.8602\n",
            "Epoch 361/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9554 - val_loss: 0.8601\n",
            "Epoch 362/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9553 - val_loss: 0.8600\n",
            "Epoch 363/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9552 - val_loss: 0.8599\n",
            "Epoch 364/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9551 - val_loss: 0.8599\n",
            "Epoch 365/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9550 - val_loss: 0.8598\n",
            "Epoch 366/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9549 - val_loss: 0.8598\n",
            "Epoch 367/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9548 - val_loss: 0.8597\n",
            "Epoch 368/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9547 - val_loss: 0.8596\n",
            "Epoch 369/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9546 - val_loss: 0.8594\n",
            "Epoch 370/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9545 - val_loss: 0.8593\n",
            "Epoch 371/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9544 - val_loss: 0.8593\n",
            "Epoch 372/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9543 - val_loss: 0.8592\n",
            "Epoch 373/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9542 - val_loss: 0.8591\n",
            "Epoch 374/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9541 - val_loss: 0.8590\n",
            "Epoch 375/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9540 - val_loss: 0.8590\n",
            "Epoch 376/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9539 - val_loss: 0.8589\n",
            "Epoch 377/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9538 - val_loss: 0.8589\n",
            "Epoch 378/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9537 - val_loss: 0.8588\n",
            "Epoch 379/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9536 - val_loss: 0.8588\n",
            "Epoch 380/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9536 - val_loss: 0.8587\n",
            "Epoch 381/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9535 - val_loss: 0.8587\n",
            "Epoch 382/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9533 - val_loss: 0.8585\n",
            "Epoch 383/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9533 - val_loss: 0.8585\n",
            "Epoch 384/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9532 - val_loss: 0.8584\n",
            "Epoch 385/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9531 - val_loss: 0.8583\n",
            "Epoch 386/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9530 - val_loss: 0.8581\n",
            "Epoch 387/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9529 - val_loss: 0.8581\n",
            "Epoch 388/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9528 - val_loss: 0.8581\n",
            "Epoch 389/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9527 - val_loss: 0.8581\n",
            "Epoch 390/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9526 - val_loss: 0.8579\n",
            "Epoch 391/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9525 - val_loss: 0.8578\n",
            "Epoch 392/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9524 - val_loss: 0.8577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 393/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9524 - val_loss: 0.8577\n",
            "Epoch 394/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9523 - val_loss: 0.8576\n",
            "Epoch 395/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9522 - val_loss: 0.8576\n",
            "Epoch 396/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9521 - val_loss: 0.8575\n",
            "Epoch 397/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9520 - val_loss: 0.8574\n",
            "Epoch 398/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9519 - val_loss: 0.8574\n",
            "Epoch 399/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9519 - val_loss: 0.8573\n",
            "Epoch 400/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9517 - val_loss: 0.8572\n",
            "Epoch 401/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9517 - val_loss: 0.8570\n",
            "Epoch 402/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9516 - val_loss: 0.8570\n",
            "Epoch 403/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9515 - val_loss: 0.8568\n",
            "Epoch 404/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9513 - val_loss: 0.8568\n",
            "Epoch 405/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9510 - val_loss: 0.8569\n",
            "Epoch 406/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9511 - val_loss: 0.8568\n",
            "Epoch 407/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9509 - val_loss: 0.8566\n",
            "Epoch 408/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9507 - val_loss: 0.8567\n",
            "Epoch 409/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9506 - val_loss: 0.8566\n",
            "Epoch 410/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9506 - val_loss: 0.8565\n",
            "Epoch 411/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9505 - val_loss: 0.8564\n",
            "Epoch 412/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9504 - val_loss: 0.8563\n",
            "Epoch 413/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9503 - val_loss: 0.8564\n",
            "Epoch 414/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9502 - val_loss: 0.8562\n",
            "Epoch 415/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9501 - val_loss: 0.8561\n",
            "Epoch 416/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9500 - val_loss: 0.8560\n",
            "Epoch 417/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9500 - val_loss: 0.8558\n",
            "Epoch 418/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9499 - val_loss: 0.8559\n",
            "Epoch 419/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9498 - val_loss: 0.8558\n",
            "Epoch 420/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9497 - val_loss: 0.8557\n",
            "Epoch 421/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9496 - val_loss: 0.8557\n",
            "Epoch 422/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9495 - val_loss: 0.8557\n",
            "Epoch 423/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9495 - val_loss: 0.8556\n",
            "Epoch 424/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9494 - val_loss: 0.8555\n",
            "Epoch 425/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9493 - val_loss: 0.8555\n",
            "Epoch 426/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9492 - val_loss: 0.8555\n",
            "Epoch 427/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9491 - val_loss: 0.8554\n",
            "Epoch 428/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9490 - val_loss: 0.8552\n",
            "Epoch 429/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9490 - val_loss: 0.8551\n",
            "Epoch 430/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9489 - val_loss: 0.8550\n",
            "Epoch 431/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9488 - val_loss: 0.8549\n",
            "Epoch 432/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9487 - val_loss: 0.8550\n",
            "Epoch 433/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9486 - val_loss: 0.8549\n",
            "Epoch 434/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9486 - val_loss: 0.8550\n",
            "Epoch 435/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9484 - val_loss: 0.8548\n",
            "Epoch 436/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9483 - val_loss: 0.8547\n",
            "Epoch 437/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9482 - val_loss: 0.8546\n",
            "Epoch 438/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9481 - val_loss: 0.8547\n",
            "Epoch 439/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9480 - val_loss: 0.8545\n",
            "Epoch 440/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9480 - val_loss: 0.8544\n",
            "Epoch 441/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9479 - val_loss: 0.8545\n",
            "Epoch 442/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9479 - val_loss: 0.8544\n",
            "Epoch 443/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9478 - val_loss: 0.8543\n",
            "Epoch 444/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9477 - val_loss: 0.8542\n",
            "Epoch 445/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9476 - val_loss: 0.8542\n",
            "Epoch 446/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9475 - val_loss: 0.8540\n",
            "Epoch 447/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9475 - val_loss: 0.8541\n",
            "Epoch 448/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9474 - val_loss: 0.8540\n",
            "Epoch 449/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9474 - val_loss: 0.8539\n",
            "Epoch 450/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9472 - val_loss: 0.8539\n",
            "Epoch 451/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9473 - val_loss: 0.8538\n",
            "Epoch 452/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9471 - val_loss: 0.8537\n",
            "Epoch 453/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9470 - val_loss: 0.8536\n",
            "Epoch 454/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9469 - val_loss: 0.8536\n",
            "Epoch 455/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9469 - val_loss: 0.8537\n",
            "Epoch 456/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9468 - val_loss: 0.8537\n",
            "Epoch 457/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9467 - val_loss: 0.8536\n",
            "Epoch 458/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9466 - val_loss: 0.8533\n",
            "Epoch 459/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9465 - val_loss: 0.8533\n",
            "Epoch 460/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9466 - val_loss: 0.8532\n",
            "Epoch 461/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9464 - val_loss: 0.8532\n",
            "Epoch 462/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9463 - val_loss: 0.8533\n",
            "Epoch 463/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9463 - val_loss: 0.8532\n",
            "Epoch 464/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9462 - val_loss: 0.8532\n",
            "Epoch 465/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9461 - val_loss: 0.8530\n",
            "Epoch 466/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9460 - val_loss: 0.8530\n",
            "Epoch 467/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9460 - val_loss: 0.8530\n",
            "Epoch 468/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9459 - val_loss: 0.8529\n",
            "Epoch 469/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9458 - val_loss: 0.8528\n",
            "Epoch 470/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9458 - val_loss: 0.8528\n",
            "Epoch 471/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9457 - val_loss: 0.8527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 472/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9456 - val_loss: 0.8527\n",
            "Epoch 473/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9456 - val_loss: 0.8526\n",
            "Epoch 474/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9456 - val_loss: 0.8524\n",
            "Epoch 475/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9454 - val_loss: 0.8524\n",
            "Epoch 476/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9454 - val_loss: 0.8524\n",
            "Epoch 477/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9453 - val_loss: 0.8524\n",
            "Epoch 478/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9453 - val_loss: 0.8523\n",
            "Epoch 479/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9452 - val_loss: 0.8523\n",
            "Epoch 480/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9451 - val_loss: 0.8521\n",
            "Epoch 481/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9451 - val_loss: 0.8521\n",
            "Epoch 482/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9450 - val_loss: 0.8521\n",
            "Epoch 483/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9449 - val_loss: 0.8522\n",
            "Epoch 484/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9449 - val_loss: 0.8523\n",
            "Epoch 485/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9448 - val_loss: 0.8521\n",
            "Epoch 486/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9448 - val_loss: 0.8520\n",
            "Epoch 487/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9447 - val_loss: 0.8520\n",
            "Epoch 488/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9446 - val_loss: 0.8519\n",
            "Epoch 489/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9446 - val_loss: 0.8519\n",
            "Epoch 490/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9445 - val_loss: 0.8519\n",
            "Epoch 491/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9445 - val_loss: 0.8518\n",
            "Epoch 492/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9444 - val_loss: 0.8518\n",
            "Epoch 493/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9443 - val_loss: 0.8517\n",
            "Epoch 494/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9443 - val_loss: 0.8518\n",
            "Epoch 495/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9442 - val_loss: 0.8517\n",
            "Epoch 496/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9441 - val_loss: 0.8514\n",
            "Epoch 497/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9441 - val_loss: 0.8513\n",
            "Epoch 498/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9440 - val_loss: 0.8513\n",
            "Epoch 499/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9440 - val_loss: 0.8513\n",
            "Epoch 500/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9439 - val_loss: 0.8513\n",
            "Epoch 501/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9438 - val_loss: 0.8513\n",
            "Epoch 502/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9438 - val_loss: 0.8513\n",
            "Epoch 503/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9437 - val_loss: 0.8513\n",
            "Epoch 504/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9437 - val_loss: 0.8513\n",
            "Epoch 505/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9436 - val_loss: 0.8512\n",
            "Epoch 506/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9435 - val_loss: 0.8510\n",
            "Epoch 507/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9435 - val_loss: 0.8510\n",
            "Epoch 508/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9435 - val_loss: 0.8508\n",
            "Epoch 509/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9434 - val_loss: 0.8509\n",
            "Epoch 510/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9433 - val_loss: 0.8509\n",
            "Epoch 511/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9433 - val_loss: 0.8509\n",
            "Epoch 512/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9432 - val_loss: 0.8509\n",
            "Epoch 513/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9432 - val_loss: 0.8508\n",
            "Epoch 514/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9431 - val_loss: 0.8507\n",
            "Epoch 515/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9431 - val_loss: 0.8506\n",
            "Epoch 516/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9430 - val_loss: 0.8505\n",
            "Epoch 517/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9429 - val_loss: 0.8505\n",
            "Epoch 518/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9429 - val_loss: 0.8505\n",
            "Epoch 519/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9428 - val_loss: 0.8506\n",
            "Epoch 520/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9428 - val_loss: 0.8506\n",
            "Epoch 521/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9427 - val_loss: 0.8504\n",
            "Epoch 522/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9427 - val_loss: 0.8504\n",
            "Epoch 523/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9426 - val_loss: 0.8503\n",
            "Epoch 524/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9426 - val_loss: 0.8503\n",
            "Epoch 525/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9426 - val_loss: 0.8502\n",
            "Epoch 526/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9425 - val_loss: 0.8502\n",
            "Epoch 527/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9425 - val_loss: 0.8501\n",
            "Epoch 528/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9425 - val_loss: 0.8503\n",
            "Epoch 529/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9423 - val_loss: 0.8502\n",
            "Epoch 530/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9423 - val_loss: 0.8499\n",
            "Epoch 531/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9423 - val_loss: 0.8498\n",
            "Epoch 532/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9422 - val_loss: 0.8498\n",
            "Epoch 533/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9421 - val_loss: 0.8499\n",
            "Epoch 534/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9421 - val_loss: 0.8499\n",
            "Epoch 535/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9420 - val_loss: 0.8500\n",
            "Epoch 536/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9422 - val_loss: 0.8498\n",
            "Epoch 537/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9421 - val_loss: 0.8497\n",
            "Epoch 538/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9420 - val_loss: 0.8498\n",
            "Epoch 539/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9418 - val_loss: 0.8496\n",
            "Epoch 540/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9419 - val_loss: 0.8496\n",
            "Epoch 541/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9418 - val_loss: 0.8496\n",
            "Epoch 542/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9417 - val_loss: 0.8494\n",
            "Epoch 543/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9417 - val_loss: 0.8495\n",
            "Epoch 544/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9416 - val_loss: 0.8495\n",
            "Epoch 545/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9416 - val_loss: 0.8494\n",
            "Epoch 546/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9416 - val_loss: 0.8493\n",
            "Epoch 547/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9415 - val_loss: 0.8493\n",
            "Epoch 548/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9415 - val_loss: 0.8493\n",
            "Epoch 549/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9414 - val_loss: 0.8491\n",
            "Epoch 550/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9413 - val_loss: 0.8492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 551/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9413 - val_loss: 0.8491\n",
            "Epoch 552/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9413 - val_loss: 0.8492\n",
            "Epoch 553/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9412 - val_loss: 0.8490\n",
            "Epoch 554/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9412 - val_loss: 0.8490\n",
            "Epoch 555/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9411 - val_loss: 0.8490\n",
            "Epoch 556/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9411 - val_loss: 0.8491\n",
            "Epoch 557/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9411 - val_loss: 0.8489\n",
            "Epoch 558/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9410 - val_loss: 0.8489\n",
            "Epoch 559/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9410 - val_loss: 0.8490\n",
            "Epoch 560/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9410 - val_loss: 0.8490\n",
            "Epoch 561/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9409 - val_loss: 0.8489\n",
            "Epoch 562/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9409 - val_loss: 0.8487\n",
            "Epoch 563/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9409 - val_loss: 0.8489\n",
            "Epoch 564/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9408 - val_loss: 0.8486\n",
            "Epoch 565/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9407 - val_loss: 0.8486\n",
            "Epoch 566/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9407 - val_loss: 0.8488\n",
            "Epoch 567/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9406 - val_loss: 0.8487\n",
            "Epoch 568/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9406 - val_loss: 0.8488\n",
            "Epoch 569/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9405 - val_loss: 0.8486\n",
            "Epoch 570/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9405 - val_loss: 0.8486\n",
            "Epoch 571/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9405 - val_loss: 0.8484\n",
            "Epoch 572/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9404 - val_loss: 0.8484\n",
            "Epoch 573/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9403 - val_loss: 0.8483\n",
            "Epoch 574/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9402 - val_loss: 0.8481\n",
            "Epoch 575/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9388 - val_loss: 0.8447\n",
            "Epoch 576/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9330 - val_loss: 0.8383\n",
            "Epoch 577/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9262 - val_loss: 0.8320\n",
            "Epoch 578/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9185 - val_loss: 0.8250\n",
            "Epoch 579/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9093 - val_loss: 0.8181\n",
            "Epoch 580/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.9027 - val_loss: 0.8134\n",
            "Epoch 581/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8967 - val_loss: 0.8086\n",
            "Epoch 582/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8919 - val_loss: 0.8049\n",
            "Epoch 583/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8879 - val_loss: 0.8020\n",
            "Epoch 584/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8844 - val_loss: 0.7994\n",
            "Epoch 585/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8814 - val_loss: 0.7973\n",
            "Epoch 586/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8787 - val_loss: 0.7951\n",
            "Epoch 587/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8762 - val_loss: 0.7934\n",
            "Epoch 588/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8742 - val_loss: 0.7916\n",
            "Epoch 589/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8721 - val_loss: 0.7894\n",
            "Epoch 590/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8701 - val_loss: 0.7880\n",
            "Epoch 591/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8681 - val_loss: 0.7865\n",
            "Epoch 592/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8665 - val_loss: 0.7845\n",
            "Epoch 593/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8640 - val_loss: 0.7819\n",
            "Epoch 594/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8614 - val_loss: 0.7810\n",
            "Epoch 595/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8593 - val_loss: 0.7776\n",
            "Epoch 596/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8570 - val_loss: 0.7760\n",
            "Epoch 597/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8549 - val_loss: 0.7744\n",
            "Epoch 598/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8528 - val_loss: 0.7723\n",
            "Epoch 599/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8506 - val_loss: 0.7707\n",
            "Epoch 600/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8487 - val_loss: 0.7692\n",
            "Epoch 601/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8469 - val_loss: 0.7676\n",
            "Epoch 602/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8451 - val_loss: 0.7663\n",
            "Epoch 603/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8435 - val_loss: 0.7651\n",
            "Epoch 604/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8419 - val_loss: 0.7638\n",
            "Epoch 605/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8405 - val_loss: 0.7628\n",
            "Epoch 606/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8391 - val_loss: 0.7616\n",
            "Epoch 607/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8377 - val_loss: 0.7612\n",
            "Epoch 608/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8369 - val_loss: 0.7596\n",
            "Epoch 609/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8353 - val_loss: 0.7593\n",
            "Epoch 610/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8341 - val_loss: 0.7577\n",
            "Epoch 611/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8326 - val_loss: 0.7570\n",
            "Epoch 612/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8313 - val_loss: 0.7560\n",
            "Epoch 613/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8302 - val_loss: 0.7552\n",
            "Epoch 614/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8291 - val_loss: 0.7543\n",
            "Epoch 615/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8280 - val_loss: 0.7533\n",
            "Epoch 616/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8270 - val_loss: 0.7525\n",
            "Epoch 617/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8260 - val_loss: 0.7515\n",
            "Epoch 618/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8250 - val_loss: 0.7507\n",
            "Epoch 619/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8240 - val_loss: 0.7499\n",
            "Epoch 620/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8231 - val_loss: 0.7492\n",
            "Epoch 621/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8222 - val_loss: 0.7485\n",
            "Epoch 622/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8213 - val_loss: 0.7478\n",
            "Epoch 623/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8206 - val_loss: 0.7469\n",
            "Epoch 624/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8196 - val_loss: 0.7465\n",
            "Epoch 625/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8189 - val_loss: 0.7460\n",
            "Epoch 626/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8182 - val_loss: 0.7452\n",
            "Epoch 627/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8173 - val_loss: 0.7444\n",
            "Epoch 628/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8165 - val_loss: 0.7439\n",
            "Epoch 629/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8158 - val_loss: 0.7433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 630/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8150 - val_loss: 0.7425\n",
            "Epoch 631/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8143 - val_loss: 0.7420\n",
            "Epoch 632/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8140 - val_loss: 0.7417\n",
            "Epoch 633/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8131 - val_loss: 0.7410\n",
            "Epoch 634/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8124 - val_loss: 0.7405\n",
            "Epoch 635/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8116 - val_loss: 0.7399\n",
            "Epoch 636/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8111 - val_loss: 0.7394\n",
            "Epoch 637/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8104 - val_loss: 0.7389\n",
            "Epoch 638/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8098 - val_loss: 0.7385\n",
            "Epoch 639/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8092 - val_loss: 0.7380\n",
            "Epoch 640/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8086 - val_loss: 0.7375\n",
            "Epoch 641/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8080 - val_loss: 0.7371\n",
            "Epoch 642/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8074 - val_loss: 0.7366\n",
            "Epoch 643/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8068 - val_loss: 0.7362\n",
            "Epoch 644/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8062 - val_loss: 0.7355\n",
            "Epoch 645/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8055 - val_loss: 0.7349\n",
            "Epoch 646/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8056 - val_loss: 0.7373\n",
            "Epoch 647/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8066 - val_loss: 0.7356\n",
            "Epoch 648/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8047 - val_loss: 0.7334\n",
            "Epoch 649/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8037 - val_loss: 0.7330\n",
            "Epoch 650/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8029 - val_loss: 0.7323\n",
            "Epoch 651/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8022 - val_loss: 0.7319\n",
            "Epoch 652/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8018 - val_loss: 0.7313\n",
            "Epoch 653/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8013 - val_loss: 0.7308\n",
            "Epoch 654/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8008 - val_loss: 0.7306\n",
            "Epoch 655/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.8003 - val_loss: 0.7303\n",
            "Epoch 656/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7999 - val_loss: 0.7298\n",
            "Epoch 657/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7994 - val_loss: 0.7293\n",
            "Epoch 658/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7990 - val_loss: 0.7290\n",
            "Epoch 659/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7986 - val_loss: 0.7287\n",
            "Epoch 660/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7982 - val_loss: 0.7286\n",
            "Epoch 661/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7978 - val_loss: 0.7280\n",
            "Epoch 662/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7974 - val_loss: 0.7279\n",
            "Epoch 663/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7970 - val_loss: 0.7275\n",
            "Epoch 664/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7966 - val_loss: 0.7272\n",
            "Epoch 665/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7962 - val_loss: 0.7272\n",
            "Epoch 666/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7959 - val_loss: 0.7267\n",
            "Epoch 667/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7955 - val_loss: 0.7264\n",
            "Epoch 668/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7951 - val_loss: 0.7262\n",
            "Epoch 669/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7948 - val_loss: 0.7259\n",
            "Epoch 670/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7944 - val_loss: 0.7256\n",
            "Epoch 671/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7941 - val_loss: 0.7253\n",
            "Epoch 672/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7937 - val_loss: 0.7252\n",
            "Epoch 673/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7934 - val_loss: 0.7248\n",
            "Epoch 674/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7930 - val_loss: 0.7245\n",
            "Epoch 675/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7926 - val_loss: 0.7243\n",
            "Epoch 676/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7927 - val_loss: 0.7243\n",
            "Epoch 677/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7923 - val_loss: 0.7239\n",
            "Epoch 678/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7918 - val_loss: 0.7234\n",
            "Epoch 679/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7915 - val_loss: 0.7231\n",
            "Epoch 680/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7911 - val_loss: 0.7229\n",
            "Epoch 681/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7908 - val_loss: 0.7227\n",
            "Epoch 682/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7905 - val_loss: 0.7226\n",
            "Epoch 683/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7902 - val_loss: 0.7224\n",
            "Epoch 684/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7898 - val_loss: 0.7220\n",
            "Epoch 685/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7898 - val_loss: 0.7217\n",
            "Epoch 686/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7893 - val_loss: 0.7215\n",
            "Epoch 687/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7891 - val_loss: 0.7214\n",
            "Epoch 688/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7888 - val_loss: 0.7212\n",
            "Epoch 689/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7884 - val_loss: 0.7211\n",
            "Epoch 690/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7882 - val_loss: 0.7207\n",
            "Epoch 691/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7881 - val_loss: 0.7206\n",
            "Epoch 692/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7876 - val_loss: 0.7201\n",
            "Epoch 693/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7878 - val_loss: 0.7199\n",
            "Epoch 694/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7872 - val_loss: 0.7194\n",
            "Epoch 695/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7868 - val_loss: 0.7194\n",
            "Epoch 696/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7862 - val_loss: 0.7191\n",
            "Epoch 697/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7860 - val_loss: 0.7188\n",
            "Epoch 698/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7857 - val_loss: 0.7185\n",
            "Epoch 699/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7854 - val_loss: 0.7185\n",
            "Epoch 700/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7852 - val_loss: 0.7182\n",
            "Epoch 701/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7851 - val_loss: 0.7179\n",
            "Epoch 702/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7848 - val_loss: 0.7176\n",
            "Epoch 703/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7844 - val_loss: 0.7177\n",
            "Epoch 704/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7842 - val_loss: 0.7173\n",
            "Epoch 705/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7840 - val_loss: 0.7170\n",
            "Epoch 706/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7836 - val_loss: 0.7168\n",
            "Epoch 707/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7834 - val_loss: 0.7168\n",
            "Epoch 708/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7832 - val_loss: 0.7163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 709/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7829 - val_loss: 0.7161\n",
            "Epoch 710/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7828 - val_loss: 0.7162\n",
            "Epoch 711/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7826 - val_loss: 0.7159\n",
            "Epoch 712/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7824 - val_loss: 0.7156\n",
            "Epoch 713/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7821 - val_loss: 0.7157\n",
            "Epoch 714/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7819 - val_loss: 0.7154\n",
            "Epoch 715/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7816 - val_loss: 0.7152\n",
            "Epoch 716/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7814 - val_loss: 0.7150\n",
            "Epoch 717/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7812 - val_loss: 0.7146\n",
            "Epoch 718/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7810 - val_loss: 0.7146\n",
            "Epoch 719/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7808 - val_loss: 0.7144\n",
            "Epoch 720/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7805 - val_loss: 0.7141\n",
            "Epoch 721/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7804 - val_loss: 0.7140\n",
            "Epoch 722/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7802 - val_loss: 0.7139\n",
            "Epoch 723/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7799 - val_loss: 0.7137\n",
            "Epoch 724/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7796 - val_loss: 0.7133\n",
            "Epoch 725/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7794 - val_loss: 0.7132\n",
            "Epoch 726/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7792 - val_loss: 0.7130\n",
            "Epoch 727/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7790 - val_loss: 0.7128\n",
            "Epoch 728/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7789 - val_loss: 0.7127\n",
            "Epoch 729/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7786 - val_loss: 0.7124\n",
            "Epoch 730/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7785 - val_loss: 0.7120\n",
            "Epoch 731/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7783 - val_loss: 0.7120\n",
            "Epoch 732/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7781 - val_loss: 0.7123\n",
            "Epoch 733/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7779 - val_loss: 0.7118\n",
            "Epoch 734/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7777 - val_loss: 0.7118\n",
            "Epoch 735/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7775 - val_loss: 0.7117\n",
            "Epoch 736/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7773 - val_loss: 0.7111\n",
            "Epoch 737/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7771 - val_loss: 0.7109\n",
            "Epoch 738/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7769 - val_loss: 0.7109\n",
            "Epoch 739/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7767 - val_loss: 0.7107\n",
            "Epoch 740/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7766 - val_loss: 0.7104\n",
            "Epoch 741/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7764 - val_loss: 0.7104\n",
            "Epoch 742/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7762 - val_loss: 0.7102\n",
            "Epoch 743/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7760 - val_loss: 0.7102\n",
            "Epoch 744/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7759 - val_loss: 0.7100\n",
            "Epoch 745/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7758 - val_loss: 0.7097\n",
            "Epoch 746/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7755 - val_loss: 0.7097\n",
            "Epoch 747/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7754 - val_loss: 0.7096\n",
            "Epoch 748/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7753 - val_loss: 0.7095\n",
            "Epoch 749/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7753 - val_loss: 0.7094\n",
            "Epoch 750/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7751 - val_loss: 0.7095\n",
            "Epoch 751/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7749 - val_loss: 0.7094\n",
            "Epoch 752/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7749 - val_loss: 0.7092\n",
            "Epoch 753/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7746 - val_loss: 0.7091\n",
            "Epoch 754/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7744 - val_loss: 0.7086\n",
            "Epoch 755/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7741 - val_loss: 0.7085\n",
            "Epoch 756/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7738 - val_loss: 0.7082\n",
            "Epoch 757/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7736 - val_loss: 0.7081\n",
            "Epoch 758/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7734 - val_loss: 0.7078\n",
            "Epoch 759/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7733 - val_loss: 0.7075\n",
            "Epoch 760/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7731 - val_loss: 0.7076\n",
            "Epoch 761/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7729 - val_loss: 0.7074\n",
            "Epoch 762/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7727 - val_loss: 0.7071\n",
            "Epoch 763/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7726 - val_loss: 0.7068\n",
            "Epoch 764/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7724 - val_loss: 0.7070\n",
            "Epoch 765/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7722 - val_loss: 0.7067\n",
            "Epoch 766/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7721 - val_loss: 0.7067\n",
            "Epoch 767/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7719 - val_loss: 0.7065\n",
            "Epoch 768/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7717 - val_loss: 0.7062\n",
            "Epoch 769/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7716 - val_loss: 0.7061\n",
            "Epoch 770/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7714 - val_loss: 0.7057\n",
            "Epoch 771/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7713 - val_loss: 0.7056\n",
            "Epoch 772/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7711 - val_loss: 0.7054\n",
            "Epoch 773/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7709 - val_loss: 0.7054\n",
            "Epoch 774/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7708 - val_loss: 0.7051\n",
            "Epoch 775/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7706 - val_loss: 0.7050\n",
            "Epoch 776/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7704 - val_loss: 0.7049\n",
            "Epoch 777/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7703 - val_loss: 0.7049\n",
            "Epoch 778/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7701 - val_loss: 0.7045\n",
            "Epoch 779/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7700 - val_loss: 0.7045\n",
            "Epoch 780/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7698 - val_loss: 0.7044\n",
            "Epoch 781/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7697 - val_loss: 0.7044\n",
            "Epoch 782/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7696 - val_loss: 0.7042\n",
            "Epoch 783/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7694 - val_loss: 0.7038\n",
            "Epoch 784/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7692 - val_loss: 0.7036\n",
            "Epoch 785/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7690 - val_loss: 0.7033\n",
            "Epoch 786/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7689 - val_loss: 0.7032\n",
            "Epoch 787/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7688 - val_loss: 0.7031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 788/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7686 - val_loss: 0.7030\n",
            "Epoch 789/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7684 - val_loss: 0.7027\n",
            "Epoch 790/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7683 - val_loss: 0.7027\n",
            "Epoch 791/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7681 - val_loss: 0.7026\n",
            "Epoch 792/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7680 - val_loss: 0.7027\n",
            "Epoch 793/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7682 - val_loss: 0.7024\n",
            "Epoch 794/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7679 - val_loss: 0.7021\n",
            "Epoch 795/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7682 - val_loss: 0.7028\n",
            "Epoch 796/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7685 - val_loss: 0.7039\n",
            "Epoch 797/1000\n",
            "380/380 [==============================] - 1s 2ms/step - loss: 0.7689 - val_loss: 0.7102\n",
            "Epoch 00797: early stopping\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['val_loss', 'loss'])\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x1604ca53240>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": [
              "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\n"
            ],
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "date_info = pd.read_csv('./data/sample_submission.csv') \n",
        "date_info.head(39)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>visitors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-03</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-04</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-05</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-06</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-07</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-08</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-05-31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 id  visitors\n",
              "0   air_00a91d42b08b08d9_2017-04-23         0\n",
              "1   air_00a91d42b08b08d9_2017-04-24         0\n",
              "2   air_00a91d42b08b08d9_2017-04-25         0\n",
              "3   air_00a91d42b08b08d9_2017-04-26         0\n",
              "4   air_00a91d42b08b08d9_2017-04-27         0\n",
              "5   air_00a91d42b08b08d9_2017-04-28         0\n",
              "6   air_00a91d42b08b08d9_2017-04-29         0\n",
              "7   air_00a91d42b08b08d9_2017-04-30         0\n",
              "8   air_00a91d42b08b08d9_2017-05-01         0\n",
              "9   air_00a91d42b08b08d9_2017-05-02         0\n",
              "10  air_00a91d42b08b08d9_2017-05-03         0\n",
              "11  air_00a91d42b08b08d9_2017-05-04         0\n",
              "12  air_00a91d42b08b08d9_2017-05-05         0\n",
              "13  air_00a91d42b08b08d9_2017-05-06         0\n",
              "14  air_00a91d42b08b08d9_2017-05-07         0\n",
              "15  air_00a91d42b08b08d9_2017-05-08         0\n",
              "16  air_00a91d42b08b08d9_2017-05-09         0\n",
              "17  air_00a91d42b08b08d9_2017-05-10         0\n",
              "18  air_00a91d42b08b08d9_2017-05-11         0\n",
              "19  air_00a91d42b08b08d9_2017-05-12         0\n",
              "20  air_00a91d42b08b08d9_2017-05-13         0\n",
              "21  air_00a91d42b08b08d9_2017-05-14         0\n",
              "22  air_00a91d42b08b08d9_2017-05-15         0\n",
              "23  air_00a91d42b08b08d9_2017-05-16         0\n",
              "24  air_00a91d42b08b08d9_2017-05-17         0\n",
              "25  air_00a91d42b08b08d9_2017-05-18         0\n",
              "26  air_00a91d42b08b08d9_2017-05-19         0\n",
              "27  air_00a91d42b08b08d9_2017-05-20         0\n",
              "28  air_00a91d42b08b08d9_2017-05-21         0\n",
              "29  air_00a91d42b08b08d9_2017-05-22         0\n",
              "30  air_00a91d42b08b08d9_2017-05-23         0\n",
              "31  air_00a91d42b08b08d9_2017-05-24         0\n",
              "32  air_00a91d42b08b08d9_2017-05-25         0\n",
              "33  air_00a91d42b08b08d9_2017-05-26         0\n",
              "34  air_00a91d42b08b08d9_2017-05-27         0\n",
              "35  air_00a91d42b08b08d9_2017-05-28         0\n",
              "36  air_00a91d42b08b08d9_2017-05-29         0\n",
              "37  air_00a91d42b08b08d9_2017-05-30         0\n",
              "38  air_00a91d42b08b08d9_2017-05-31         0"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_points = 478 - 39 - 39\n",
        "date_start_col = 1\n",
        "inp_start = train_points + date_start_col\n",
        "inp_end = inp_start + input_seq_len\n",
        "out_end = inp_end + output_seq_len \n",
        "last_train_day = data.iloc[: , inp_end:out_end]"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "last_train_day"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2017-03-15</th>\n",
              "      <th>2017-03-16</th>\n",
              "      <th>2017-03-17</th>\n",
              "      <th>2017-03-18</th>\n",
              "      <th>2017-03-19</th>\n",
              "      <th>2017-03-20</th>\n",
              "      <th>2017-03-21</th>\n",
              "      <th>2017-03-22</th>\n",
              "      <th>2017-03-23</th>\n",
              "      <th>2017-03-24</th>\n",
              "      <th>...</th>\n",
              "      <th>2017-04-13</th>\n",
              "      <th>2017-04-14</th>\n",
              "      <th>2017-04-15</th>\n",
              "      <th>2017-04-16</th>\n",
              "      <th>2017-04-17</th>\n",
              "      <th>2017-04-18</th>\n",
              "      <th>2017-04-19</th>\n",
              "      <th>2017-04-20</th>\n",
              "      <th>2017-04-21</th>\n",
              "      <th>2017-04-22</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.761200</td>\n",
              "      <td>3.688879</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>3.332205</td>\n",
              "      <td>3.555348</td>\n",
              "      <td>...</td>\n",
              "      <td>3.555348</td>\n",
              "      <td>3.688879</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>3.583519</td>\n",
              "      <td>2.890372</td>\n",
              "      <td>3.663562</td>\n",
              "      <td>4.025352</td>\n",
              "      <td>2.944439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.484907</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>2.772589</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>2.397895</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>...</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>3.295837</td>\n",
              "      <td>1.945910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.079442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>2.772589</td>\n",
              "      <td>2.397895</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>2.639057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.401197</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>2.772589</td>\n",
              "      <td>3.218876</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.302585</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.218876</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>2.197225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.944439</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>3.526361</td>\n",
              "      <td>3.761200</td>\n",
              "      <td>4.204693</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>3.091042</td>\n",
              "      <td>3.135494</td>\n",
              "      <td>...</td>\n",
              "      <td>3.135494</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>3.465736</td>\n",
              "      <td>3.688879</td>\n",
              "      <td>3.258097</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>3.465736</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>3.583519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>816</th>\n",
              "      <td>3.295837</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>3.091042</td>\n",
              "      <td>3.135494</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>...</td>\n",
              "      <td>2.890372</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>2.772589</td>\n",
              "      <td>2.302585</td>\n",
              "      <td>2.833213</td>\n",
              "      <td>2.890372</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>2.708050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>2.995732</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>2.708050</td>\n",
              "      <td>2.833213</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>2.708050</td>\n",
              "      <td>3.295837</td>\n",
              "      <td>3.332205</td>\n",
              "      <td>3.091042</td>\n",
              "      <td>3.433987</td>\n",
              "      <td>...</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>3.871201</td>\n",
              "      <td>3.496508</td>\n",
              "      <td>2.772589</td>\n",
              "      <td>3.135494</td>\n",
              "      <td>3.496508</td>\n",
              "      <td>3.295837</td>\n",
              "      <td>3.332205</td>\n",
              "      <td>3.988984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>2.639057</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>...</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>2.302585</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>1.791759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>2.995732</td>\n",
              "      <td>3.332205</td>\n",
              "      <td>3.433987</td>\n",
              "      <td>2.944439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>3.496508</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>3.526361</td>\n",
              "      <td>...</td>\n",
              "      <td>2.833213</td>\n",
              "      <td>3.637586</td>\n",
              "      <td>3.258097</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.708050</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>3.367296</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>4.007333</td>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>2.079442</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>1.791759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>821 rows × 39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     2017-03-15  2017-03-16  2017-03-17  2017-03-18  2017-03-19  2017-03-20  \\\n",
              "0      3.761200    3.688879    3.637586    1.609438    0.000000    0.000000   \n",
              "1      2.484907    1.791759    2.772589    1.945910    0.000000    0.000000   \n",
              "2      2.079442    0.000000    1.945910    2.639057    2.564949    2.079442   \n",
              "3      0.000000    1.945910    1.791759    0.000000    3.401197    2.197225   \n",
              "4      2.944439    3.044522    2.944439    3.526361    3.761200    4.204693   \n",
              "..          ...         ...         ...         ...         ...         ...   \n",
              "816    3.295837    2.079442    3.367296    3.178054    2.079442    0.000000   \n",
              "817    2.995732    3.367296    2.708050    2.833213    3.367296    2.708050   \n",
              "818    2.639057    1.791759    1.791759    2.079442    2.639057    2.079442   \n",
              "819    2.995732    3.332205    3.433987    2.944439    0.000000    0.000000   \n",
              "820    2.079442    1.386294    2.197225    2.197225    1.098612    1.098612   \n",
              "\n",
              "     2017-03-21  2017-03-22  2017-03-23  2017-03-24  ...  2017-04-13  \\\n",
              "0      3.367296    3.178054    3.332205    3.555348  ...    3.555348   \n",
              "1      0.000000    2.944439    2.397895    2.944439  ...    2.639057   \n",
              "2      1.945910    1.609438    0.000000    2.197225  ...    0.000000   \n",
              "3      2.639057    2.772589    3.218876    0.000000  ...    0.000000   \n",
              "4      2.564949    2.079442    3.091042    3.135494  ...    3.135494   \n",
              "..          ...         ...         ...         ...  ...         ...   \n",
              "816    2.197225    3.091042    3.135494    1.945910  ...    2.890372   \n",
              "817    3.295837    3.332205    3.091042    3.433987  ...    2.484907   \n",
              "818    0.000000    1.386294    2.197225    2.079442  ...    1.945910   \n",
              "819    2.564949    3.496508    3.044522    3.526361  ...    2.833213   \n",
              "820    1.098612    0.693147    1.386294    2.197225  ...    0.693147   \n",
              "\n",
              "     2017-04-14  2017-04-15  2017-04-16  2017-04-17  2017-04-18  2017-04-19  \\\n",
              "0      3.688879    0.000000    0.000000    2.995732    3.583519    2.890372   \n",
              "1      2.079442    0.693147    0.000000    1.098612    0.693147    2.197225   \n",
              "2      1.609438    2.772589    2.397895    2.564949    2.995732    2.197225   \n",
              "3      2.302585    1.609438    1.386294    1.386294    0.000000    3.218876   \n",
              "4      2.944439    3.465736    3.688879    3.258097    3.044522    3.465736   \n",
              "..          ...         ...         ...         ...         ...         ...   \n",
              "816    3.044522    3.044522    2.772589    2.302585    2.833213    2.890372   \n",
              "817    3.637586    3.871201    3.496508    2.772589    3.135494    3.496508   \n",
              "818    0.693147    3.178054    2.302585    2.079442    0.000000    2.639057   \n",
              "819    3.637586    3.258097    0.000000    2.708050    3.367296    3.367296   \n",
              "820    1.791759    2.079442    2.079442    1.386294    1.945910    1.098612   \n",
              "\n",
              "     2017-04-20  2017-04-21  2017-04-22  \n",
              "0      3.663562    4.025352    2.944439  \n",
              "1      0.693147    3.295837    1.945910  \n",
              "2      0.000000    1.386294    2.639057  \n",
              "3      0.000000    2.995732    2.197225  \n",
              "4      2.564949    3.637586    3.583519  \n",
              "..          ...         ...         ...  \n",
              "816    3.178054    3.367296    2.708050  \n",
              "817    3.295837    3.332205    3.988984  \n",
              "818    0.693147    1.386294    1.791759  \n",
              "819    3.178054    4.007333    0.693147  \n",
              "820    1.098612    1.609438    1.791759  \n",
              "\n",
              "[821 rows x 39 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "last_train_day = last_train_day.values.reshape(1, -1, output_seq_len).transpose(0, 2, 1)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "last_train_day.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": [
              "(1, 39, 821)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "last_train_day[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": [
              "array([[3.76120012, 2.48490665, 2.07944154, ..., 2.63905733, 2.99573227,\n",
              "        2.07944154],\n",
              "       [3.68887945, 1.79175947, 0.        , ..., 1.79175947, 3.33220451,\n",
              "        1.38629436],\n",
              "       [3.63758616, 2.77258872, 1.94591015, ..., 1.79175947, 3.4339872 ,\n",
              "        2.19722458],\n",
              "       ...,\n",
              "       [3.66356165, 0.69314718, 0.        , ..., 0.69314718, 3.17805383,\n",
              "        1.09861229],\n",
              "       [4.02535169, 3.29583687, 1.38629436, ..., 1.38629436, 4.00733319,\n",
              "        1.60943791],\n",
              "       [2.94443898, 1.94591015, 2.63905733, ..., 1.79175947, 0.69314718,\n",
              "        1.79175947]])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_output = model.predict(last_train_day)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_output.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": [
              "(1, 39, 821)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_output"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": [
              "array([[[0.5598955 , 0.24129564, 1.827618  , ..., 1.6214377 ,\n",
              "         0.52164596, 0.8735753 ],\n",
              "        [2.459498  , 1.1773065 , 1.6919785 , ..., 1.7574815 ,\n",
              "         2.35845   , 1.5112444 ],\n",
              "        [2.9465075 , 1.416747  , 1.6156555 , ..., 1.7529542 ,\n",
              "         2.8299534 , 1.659169  ],\n",
              "        ...,\n",
              "        [2.39311   , 1.1456858 , 1.7824115 , ..., 1.8339998 ,\n",
              "         2.293033  , 1.5210469 ],\n",
              "        [2.934145  , 1.4108142 , 1.6290051 , ..., 1.7638925 ,\n",
              "         2.8178215 , 1.659687  ],\n",
              "        [3.0190248 , 1.4524084 , 1.6049381 , ..., 1.7528939 ,\n",
              "         2.9001522 , 1.6814375 ]]], dtype=float32)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "people = np.exp(test_output[0][:,0]) - 1"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "day = np.arange(1,40)\n",
        "plt.scatter(day,people)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x1604cdbeda0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": [
              "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\n"
            ],
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "visitor = np.exp(test_output[0]) - 1"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "visitor.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": [
              "(39, 821)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "result = visitor.T.ravel()"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": [
              "(32019,)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "datetime = pd.date_range('2017-04-23',periods=39)\n",
        "string_list = []\n",
        "for i in range(len(data['air_store_id'])): #821\n",
        "    for j in range(len(datetime)): #39\n",
        "        string_list.append(data['air_store_id'][i]+'_'+datetime[j].strftime('%Y-%m-%d'))"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "string_list[32019-39:32019]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": [
              "['air_fff68b929994bfbd_2017-04-23',\n",
              " 'air_fff68b929994bfbd_2017-04-24',\n",
              " 'air_fff68b929994bfbd_2017-04-25',\n",
              " 'air_fff68b929994bfbd_2017-04-26',\n",
              " 'air_fff68b929994bfbd_2017-04-27',\n",
              " 'air_fff68b929994bfbd_2017-04-28',\n",
              " 'air_fff68b929994bfbd_2017-04-29',\n",
              " 'air_fff68b929994bfbd_2017-04-30',\n",
              " 'air_fff68b929994bfbd_2017-05-01',\n",
              " 'air_fff68b929994bfbd_2017-05-02',\n",
              " 'air_fff68b929994bfbd_2017-05-03',\n",
              " 'air_fff68b929994bfbd_2017-05-04',\n",
              " 'air_fff68b929994bfbd_2017-05-05',\n",
              " 'air_fff68b929994bfbd_2017-05-06',\n",
              " 'air_fff68b929994bfbd_2017-05-07',\n",
              " 'air_fff68b929994bfbd_2017-05-08',\n",
              " 'air_fff68b929994bfbd_2017-05-09',\n",
              " 'air_fff68b929994bfbd_2017-05-10',\n",
              " 'air_fff68b929994bfbd_2017-05-11',\n",
              " 'air_fff68b929994bfbd_2017-05-12',\n",
              " 'air_fff68b929994bfbd_2017-05-13',\n",
              " 'air_fff68b929994bfbd_2017-05-14',\n",
              " 'air_fff68b929994bfbd_2017-05-15',\n",
              " 'air_fff68b929994bfbd_2017-05-16',\n",
              " 'air_fff68b929994bfbd_2017-05-17',\n",
              " 'air_fff68b929994bfbd_2017-05-18',\n",
              " 'air_fff68b929994bfbd_2017-05-19',\n",
              " 'air_fff68b929994bfbd_2017-05-20',\n",
              " 'air_fff68b929994bfbd_2017-05-21',\n",
              " 'air_fff68b929994bfbd_2017-05-22',\n",
              " 'air_fff68b929994bfbd_2017-05-23',\n",
              " 'air_fff68b929994bfbd_2017-05-24',\n",
              " 'air_fff68b929994bfbd_2017-05-25',\n",
              " 'air_fff68b929994bfbd_2017-05-26',\n",
              " 'air_fff68b929994bfbd_2017-05-27',\n",
              " 'air_fff68b929994bfbd_2017-05-28',\n",
              " 'air_fff68b929994bfbd_2017-05-29',\n",
              " 'air_fff68b929994bfbd_2017-05-30',\n",
              " 'air_fff68b929994bfbd_2017-05-31']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(result, index = string_list,columns=['visitors'])"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.index.name = 'id'"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visitors</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>air_00a91d42b08b08d9_2017-04-23</th>\n",
              "      <td>0.750490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_00a91d42b08b08d9_2017-04-24</th>\n",
              "      <td>10.698936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_00a91d42b08b08d9_2017-04-25</th>\n",
              "      <td>18.039341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_00a91d42b08b08d9_2017-04-26</th>\n",
              "      <td>19.507151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_00a91d42b08b08d9_2017-04-27</th>\n",
              "      <td>19.717909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_fff68b929994bfbd_2017-05-27</th>\n",
              "      <td>3.390831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_fff68b929994bfbd_2017-05-28</th>\n",
              "      <td>2.002104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_fff68b929994bfbd_2017-05-29</th>\n",
              "      <td>3.577014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_fff68b929994bfbd_2017-05-30</th>\n",
              "      <td>4.257665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_fff68b929994bfbd_2017-05-31</th>\n",
              "      <td>4.373274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32019 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  visitors\n",
              "id                                        \n",
              "air_00a91d42b08b08d9_2017-04-23   0.750490\n",
              "air_00a91d42b08b08d9_2017-04-24  10.698936\n",
              "air_00a91d42b08b08d9_2017-04-25  18.039341\n",
              "air_00a91d42b08b08d9_2017-04-26  19.507151\n",
              "air_00a91d42b08b08d9_2017-04-27  19.717909\n",
              "...                                    ...\n",
              "air_fff68b929994bfbd_2017-05-27   3.390831\n",
              "air_fff68b929994bfbd_2017-05-28   2.002104\n",
              "air_fff68b929994bfbd_2017-05-29   3.577014\n",
              "air_fff68b929994bfbd_2017-05-30   4.257665\n",
              "air_fff68b929994bfbd_2017-05-31   4.373274\n",
              "\n",
              "[32019 rows x 1 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('result2.csv')"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}